# Awesome-spatial-visual-reasoning-MLLMs

Repository for awesome spatial/visual reasoning MLLMs.  (focus more on embodied applications)

![Research Topics in Spatial-Visual Reasoning](wordcloud.png)

## Image

1. [arxiv 2505] Active-o3 : Empowering Multimodal Large Language Models with Active Perception via GRPO [[Code](https://github.com/aim-uofa/Active-o3)] 
2. [arxiv 2505] One RL to See Them All: Visual Triple Unified Reinforcement Learning [[Code](https://github.com/MiniMax-AI/One-RL-to-See-Them-All)] [[Dataset](https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k)] 
3. [arxiv 2505] DeepEyes: Incentivizing “Thinking with Images” via Reinforcement Learning [[Code](https://github.com/Visual-Agent/DeepEyes)] [[Dataset](https://huggingface.co/datasets/ChenShawn/DeepEyes-Datasets-47k)]
4. [arxiv 2505] R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO [[Code](https://github.com/HJYao00/R1-ShareVL)]
5. [arxiv 2505]  Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models [[Code](https://github.com/kokolerk/TON)] [[Datasets](https://huggingface.co/collections/kolerk/ton-682ad9038395c21e228a645b)]  
6. [arxiv2505] Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning [[Datasets](https://huggingface.co/collections/TIGER-Lab/pixel-reasoner-682fe96ea946d10dda60d24e)] [[Code](https://github.com/TIGER-AI-Lab/Pixel-Reasoner)]
7. [arxiv 2505] VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction [[Code](https://github.com/VITA-Group/VLM-3R)]
8. [arxiv 2025] G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning [[Code](https://github.com/chenllliang/G1)]
9. [arxiv 2505] SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward  [[Code](https://github.com/kxfan2002/SophiaVL-R1)] [[Dataset](https://huggingface.co/datasets/bunny127/SophiaVL-R1-Thinking-156k)]
10. [arxiv 2505] VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning [[Code](https://github.com/dvlab-research/VisionReasoner)] [[Dataset](https://huggingface.co/datasets/Ricky06662/VisionReasoner_multi_object_7k_840)]
11. [arxiv 2505] GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning [[Code](https://github.com/yueliu1999/GuardReasoner-VL)]
12. [arxiv 2505] Multi-SpatialMLLM Multi-Frame Spatial Understanding with Multi-Modal Large Language Models  [[Code](https://github.com/facebookresearch/Multi-SpatialMLLM)] 
13. [arxiv 2505] GRIT: Teaching MLLMs to Think with Images [[Code](https://github.com/eric-ai-lab/GRIT)] 
14. [arxiv 2505] Visual Agentic Reinforcement Fine-Tuning [[Code](https://github.com/Liuziyu77/Visual-RFT)] [[Dataset](https://huggingface.co/collections/laolao77/visual-arft-682c601d0e35ac6470adfe9f)]
15. [arxiv 2505] OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning  [[Code](https://github.com/zhaochen0110/OpenThinkIMG)] [[Dataset](https://huggingface.co/collections/Warrieryes/openthinkimg-68244a63e97a24d9b7ffcde9)]
16. [arxiv 2504] Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning [[Code](https://github.com/SkyworkAI/Skywork-R1V)]
17. [arxiv 2504] Perception-R1: Pioneering Perception Policy with Reinforcement Learning  [[Code](https://github.com/linkangheng/PR1)] [[Dataset](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af)]
18. [arxiv 2504] SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement  [[Code](https://github.com/si0wang/ThinkLite-VL)] [[Datasets](https://huggingface.co/collections/russwang/thinklite-vl-67f88c6493f8a7601e73fe5a)] 
19. [arxiv 2504] VLM-R1: A stable and generalizable R1-style Large Vision-Language Model [[Code](https://github.com/om-ai-lab/VLM-R1)] [[Dataset](https://huggingface.co/datasets/omlab/VLM-R1)]
20. [arxiv 2504] VLAA-Thinking: SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models [[Code](https://github.com/UCSC-VLAA/VLAA-Thinking)] [[Dataset](https://huggingface.co/datasets/UCSC-VLAA/VLAA-Thinking)]  
21. [arxiv 2504] MAYE: Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme [[Code](https://github.com/GAIR-NLP/MAYE)] [[Dataset](https://huggingface.co/datasets/ManTle/MAYE)]  
22. [arxiv 2504] R1-SGG: Compile Scene Graphs with Reinforcement Learning [[Code](https://github.com/gpt4vision/R1-SGG)]
23. [arxiv 2504] NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation [[Code](https://github.com/John-AI-Lab/NoisyRollout)] [[Datasets](https://huggingface.co/collections/xyliu6/noisyrollout-67ff992d1cf251087fe021a2)]
24. [arxiv 2504] VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning [[Code](https://github.com/TIGER-AI-Lab/VL-Rethinker)] [[Dataset](https://huggingface.co/datasets/TIGER-Lab/ViRL39K)] 
25. [arxiv 2503] Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning  [[Code](https://github.com/tanhuajie/Reason-RFT)] [[Dataset](https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset)]
26. [arxiv 2503] MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning [[Code](https://github.com/ModalMinds/MM-EUREKA)] [[Dataset](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset)]
27. [arxiv 2503] Visual-RFT: Visual Reinforcement Fine-Tuning [[Code](https://github.com/Liuziyu77/Visual-RFT)] [[Datasets](https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df)] 
28. [arxiv 2503] OpenVLThinker: An Early Exploration to Vision-Language Reasoning via Iterative Self-Improvement [[Code](https://github.com/yihedeng9/OpenVLThinker)]
29. [arxiv 2503] Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning [[Code](https://github.com/minglllli/CLS-RL)] [[Datasets](https://huggingface.co/afdsafas)] 
30. [arxiv 2503] R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization [[Code](https://github.com/jingyi0000/R1-VL)]
31. [arxiv 2503] Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought [[Code](https://github.com/SkyworkAI/Skywork-R1V)]
32. [arxiv 2503] R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization [[Code](https://github.com/Fancy-MLLM/R1-Onevision)] [[Dataset](https://huggingface.co/datasets/Fancy-MLLM/R1-Onevision)] 
33. [arxiv 2503]VisualPRM: An Effective Process Reward Model for Multimodal Reasoning [[Code](https://huggingface.co/OpenGVLab/VisualPRM-8B)]  [[Dataset](https://huggingface.co/datasets/OpenGVLab/VisualPRM400K)]
34. [arxiv 2503] LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL [[Code](https://github.com/TideDra/lmm-r1)]
35. [arxiv 2503] Curr-ReFT: Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning [[Code](https://github.com/ding523/Curr_REFT)] [[Dataset](https://huggingface.co/datasets/ZTE-AIM/Curr-ReFT-data)] 
36. [arxiv 2503] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models [[Code](https://github.com/Osilly/Vision-R1)]
37. [arxiv 2501] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[Code](https://github.com/HJYao00/Mulberry)]
38. [arxiv 2501] Virgo: A Preliminary Exploration on Reproducing o1-like MLLM [[Code](https://github.com/RUCAIBox/Virgo)]
39. [arxiv 2411] Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models [[Code](https://github.com/dongyh20/Insight-V)]
40. [arxiv 2406, ICRA'25] SpatialBot: Precise Spatial Understanding with Vision Language Models  [[Code](https://github.com/BAAI-DCAI/SpatialBot)] [[Dataset](https://huggingface.co/datasets/RussRobin/SpatialQA)]
41. [arxiv 2406, NIPS'24] SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models [[Code](https://github.com/AnjieCheng/SpatialRGPT)] [[Dataset](https://huggingface.co/datasets/a8cheng/OpenSpatialDataset)]
42. [arxiv 2405, NAACL'25 main] VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models [[Code](https://github.com/RupertLuo/VoCoT)] [[Dataset](https://huggingface.co/datasets/luoruipu1/VoCoT)]
43. [arxiv 2401,CVPR'24 main] SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities [[Code](https://github.com/remyxai/VQASynth)]



## Video

1. [arxiv 2505] Time-R1: Towards Comprehensive Temporal Reasoning in LLMs  [[Code](https://github.com/ulab-uiuc/Time-R1)] [[Dataset](https://huggingface.co/datasets/ulab-ai/Time-Bench)]
2. [arxiv 2505] SpaceR: Reinforcing MLLMs in Video Spatial Reasoning [[Code]( https://github.com/OuyangKun10/SpaceR)] [[Dataset](https://huggingface.co/datasets/RUBBISHLIKE/SpaceR-151k)]
3. [arxiv 2504] TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning [[Code](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1)]
4. [arxiv 2504] VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning [[Code](https://github.com/OpenGVLab/VideoChat-R1)]
5. [arxiv 2504] Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning [[Code](https://github.com/OuyangKun10/Spatial-R1)]
6. [arxiv 2504] R1-Zero-VSI: Improved Visual-Spatial Reasoning via R1-Zero-Like Training [[Code](https://github.com/zhijie-group/R1-Zero-VSI)]
7. [arxiv 2503] SEED-Bench-R1: Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1 [[Code](https://github.com/TencentARC/SEED-Bench-R1)] [[Dataset](https://huggingface.co/datasets/TencentARC/SEED-Bench-R1)]
8. [arxiv 2503] Video-R1: Reinforcing Video Reasoning in MLLMs [[Code](https://github.com/tulerfeng/Video-R1)] [[Dataset](https://huggingface.co/datasets/Video-R1/Video-R1-data)] 
9. [arxiv 2503] TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM [[Code](https://github.com/www-Ye/TimeZero)]

   

## Embodied

1. [arxiv 2505] InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning [[Code](https://github.com/Koorye/Inspire)] 
2. [arxiv 2505] From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems [[Code will be released soon](https://github.com/xiuchao/InstructionGrounding)]
3. [arxiv 2505] OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning [[Code](https://github.com/Fanqi-Lin/OneTwoVLA)] [[Dataset](https://huggingface.co/datasets/Richard-Nai/onetwovla-dataset)]
4. [arxiv 2505] VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning [[Code](https://github.com/GuanxingLu/vlarl)] 
5. [arxiv 2505] AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving
6. [arxiv 2505] SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning [[Code](https://github.com/yliu-cs/SSR)] [[Dataset](https://huggingface.co/collections/yliu-cs/ssr-682d44496b64e4edd94092bb)]
7. [arxiv 2504, CVPR'25] InteractVLM: 3D Interaction Reasoning from 2D Foundational Models [[Code](https://github.com/kxfan2002/SophiaVL-R1)] [[Dataset](https://huggingface.co/datasets/bunny127/SophiaVL-R1-Thinking-156k)]
8. [arxiv 2504] Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning  [[Code](https://github.com/EmbodiedCity/Embodied-R.code)] 
9. [arxiv 2504, CVPR'25] RoboGround: Robotic Manipulation with Grounded Vision-Language Priors   [[Code](https://github.com/ZzZZCHS/RoboGround)] [[Dataset](https://huggingface.co/datasets/ZzZZCHS/RoboGround_Data/tree/main)]
10. [arxiv 2503] Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks [[Code](https://github.com/zwq2018/embodied_reasoner)] [[Dataset](https://huggingface.co/datasets/zwq2018/embodied_reasoner)] 
11. [arxiv 2503] LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning? [[Code](https://github.com/Tangkexian/LEGO-Puzzles)] [[Dataset](https://huggingface.co/datasets/KexianTang/LEGO-Puzzles)]
12. [arxiv 2503] MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse [[Code](https://github.com/PzySeere/MetaSpatial)] [[Dataset](https://huggingface.co/datasets/zhenyupan/3d_layout_reasoning)] 
13. [arxiv 2503] Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning [[Code](https://github.com/nvidia-cosmos/cosmos-reason1)] [[Dataset](https://huggingface.co/collections/nvidia/cosmos-reason1-67c9e926206426008f1da1b7)] 
14. [arxiv 2502] ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy [[Code](https://github.com/cccedric/conrft)] 
15. [arxiv 2502] SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation [[Code](https://github.com/qizekun/SoFar)] 
16. [arxiv 2502] Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos
17. [arxiv 2501, RSS'25] SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Models [[Code](https://github.com/SpatialVLA/SpatialVLA)] 
18. [CVPR'24] ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation [[Code](https://github.com/clorislili/ManipLLM)] 
19. [arxiv 2412] Enhancing Multi-Robot Semantic Navigation Through Multimodal Chain-of-Thought Score Collaboration [[Code](https://github.com/FrankZxShen/MCoCoNav)] 
20. [arxiv 2412] Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection [[Code will be released soon](https://zhoues.github.io/Code-as-Monitor/)]
21. [CVPR'25 Highlight] OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints [[Code will be released soon](https://github.com/pmj110119/OmniManip)]
22. [arxiv 2407, CoRL'24] Robotic Control via Embodied Chain-of-Thought Reasoning [[Code](https://github.com/MichalZawalski/embodied-CoT/)]
23. [arxiv 2405] ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation [[Code](https://github.com/huangwl18/ReKep)]
24. [arxiv 2305, NIPS'23 Spotlight] EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought [[Code](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch/)]
25. [CoRL'24] ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter [[Code](https://github.com/H-Freax/ThinkGrasp)]
26. SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning
27. Memory-Driven Multimodal Chain of Thought for Embodied Long-Horizon Task Planning



## Audio/Omini

1. [arxiv 2505] Omni-R1 (ZJU): Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration [[Code](https://github.com/aim-uofa/Omni-R1)]
2. [arxiv 2505] Omni-R1 (MIT): Do You Really Need Audio to Fine-Tune Your Audio LLM?
3. [arxiv 2505] EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning [[Dataset](https://huggingface.co/datasets/harryhsing/OmniInstruct_V1_AVQA_R1)] [[Code](https://github.com/HarryHsing/EchoInk)]
4. [arxiv 2504] SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning
5. [arxiv 2503] R1-AQA: Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering [[Code](https://github.com/xiaomi-research/r1-aqa)]
6. [arxiv 2503] Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models [[Code](https://github.com/xzf-thu/Audio-Reasoner)]
7. [arxiv 2503] R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning  [[Code](https://github.com/HumanMLLM/R1-Omni)]


## Math

1. [arxiv 2505] URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics [[Code](https://github.com/URSA-MATH)] [[Datasets](https://huggingface.co/URSA-MATH)] 



## Web/GUI

1. [arxiv 2505] Web-Shepherd: Advancing PRMs for Reinforcing Web Agents  [[Code](https://github.com/kyle8581/Web-Shepherd)] [[Datasets](https://huggingface.co/collections/LangAGI-Lab/web-shepherd-advancing-prms-for-reinforcing-web-agents-682b4f4ad607fc27c4dc49e8)]
2. [arxiv 2505] ARPO: End-to-End Policy Optimization for GUI Agents with Experience Replay [[Code](https://github.com/dvlab-research/ARPO)]
3. [arxiv 2505] GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents [[Code](https://github.com/Yuqi-Zhou/GUI-G1)]
4. [arxiv 2504] WebThinker: Empowering Large Reasoning Models with Deep Research Capability   [[Code](https://github.com/RUC-NLPIR/WebThinker)] [[Dataset](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af)]
5. [arxiv 2504] InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners [[Code](https://github.com/Reallm-Labs/InfiGUI-R1)]
6. [arxiv 2504] GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents [[Dataset](https://huggingface.co/datasets/ritzzai/GUI-R1)]  [[Code](https://github.com/ritzz-ai/GUI-R1)]



## Medical

1. [arxiv 2503] Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models [[Code](https://github.com/Yuxiang-Lai117/Med-R1)]
2. [arxiv 2502] MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning [[Code](https://huggingface.co/JZPeterPan/MedVLM-R1)]
3. [EMNLP'24 main] MedCoT: Medical Chain of Thought via Hierarchical Expert  [[Code](https://github.com/JXLiu-AI/MedCoT)]



## Grounding

1. [arxiv 2505] UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning [[Code](https://github.com/AMAP-ML/UniVG-R1)] [[Dataset](https://huggingface.co/datasets/GD-ML/UniVG-R1-data)] 
2. [arxiv 2504] CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward [[Code](https://github.com/yeyimilk/CrowdVLM-R1)] [[Dataset](https://huggingface.co/datasets/yeyimilk/CrowdVLM-R1-data)] 
3. [arxiv 2503] Integrating Chain-of-Thought for Multimodal Alignment: A Study on 3D Vision-Language Learning [[Dataset](https://huggingface.co/datasets/Battam/3D-CoT)] 
4. [arxiv 2503] Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement [[Code](https://github.com/dvlab-research/Seg-Zero)]



### Multimodal Reward Model 

1. [arxiv 2505] Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning [[Code](https://github.com/SkyworkAI/Skywork-R1V)]
2. [arxiv 2505] UnifiedReward-Think: Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning [[Code](https://github.com/CodeGoat24/UnifiedReward)] [[Datasets](https://huggingface.co/collections/CodeGoat24/unifiedreward-training-data-67c300d4fd5eff00fa7f1ede)] 
3. [arxiv 2505] R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning [[Code](https://github.com/yfzhang114/r1_reward)] [[Dataset](https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL)]



## Evaluation for reasoning ability

1. [arxiv 2505] RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction [[Code](https://github.com/MINT-SJTU/RoboFAC)]
2. [arxiv 2505] SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding  [[Code](https://github.com/haoningwu3639/SpatialScore)]
3. [arxiv 2505] EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video [[Dataset](https://ml-site.cdn-apple.com/datasets/egodex/test.zip)]
4. [arxiv 2505] PhyX: Does Your Model Have the "Wits" for Physical Reasoning? [[Code](https://github.com/NastyMarcus/PhyX)] [[Dataset](https://huggingface.co/datasets/Cloudriver/PhyX)]
5. [arxiv 2505] ReasonMap: Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps [[Code](https://github.com/fscdc/ReasonMap)]  [[Dataset](https://huggingface.co/datasets/FSCCS/ReasonMap)] 
6. [arxiv 2504] VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models [[Dataset](https://huggingface.co/datasets/VisuLogic/VisuLogic)] [[Code](https://github.com/VisuLogic-Benchmark)] 
7. [arxiv 2504] Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark [[Dataset](https://huggingface.co/datasets/Enxin/Video-MMLU)] [[Code](https://github.com/Espere-1119-Song/Video-MMLU)] 
8. [arxiv 2504] VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning [[Dataset](https://huggingface.co/datasets/VLM-Reasoning/VCR-Bench)] [[Code](https://github.com/zhishuifeiqian/VCR-Bench)]
9. [arxiv 2504] MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models [[Code](https://github.com/LanceZPF/MDK12)]
10. [arxiv 2503] V1-33K: Toward Multimodal Reasoning by Designing Auxiliary Tasks [[Dataset](https://huggingface.co/datasets/haonan3/V1-33K)] [[Code](https://github.com/haonan3/V1)]
11. [arxiv 2502] MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models [[Dataset](https://huggingface.co/datasets/huanqia/MM-IQ)] [[Code](https://github.com/AceCHQ/MMIQ)] 
12. [arxiv 2502] MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency [[Dataset](https://huggingface.co/datasets/CaraJ/MME-CoT)] [[Code](https://github.com/CaraJ7/MME-CoT)]
13. [arxiv 2502] ZeroBench: An Impossible* Visual Benchmark for Contemporary Large Multimodal Models [[Dataset](https://huggingface.co/datasets/jonathan-roberts1/zerobench)] [[Code](https://github.com/jonathan-roberts1/zerobench/)]
14. [arxiv 2502] HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks [[Dataset](https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark)] [[Code](https://github.com/HumanEval-V/HumanEval-V-Benchmark)]
15. [arxiv 2306] LIBERO:Benchmarking Knowledge Transfer for Lifelong Robot Learning [[Code](https://github.com/Lifelong-Robot-Learning/LIBERO)]
16. [RAL'22 ] CALVIN: A benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks [[Code](https://github.com/mees/calvin)]



## Embodied Environment

1. Maniskill [[Code](https://github.com/haosulab/ManiSkill)]
2. Issac Lab/Gym/Sim [[Code](https://github.com/isaac-sim/IsaacGymEnvs)]
3. SimplerEnv [[Code](https://github.com/isaac-sim/IsaacGymEnvs)]
4. Roboverse [[code](https://github.com/simpler-env/SimplerEnv)]
5. Bullet Physics SDK [[code](https://github.com/bulletphysics/bullet3)]
6. Genesis [[Code](https://github.com/Genesis-Embodied-AI/Genesis)]
7. RoboTwin [[Code](https://github.com/TianxingChen/RoboTwin)]
8. OmniGibson [[Document](https://behavior.stanford.edu/omnigibson/getting_started/installation.html)]
9. Gazebo [[Link](https://gazebosim.org/)]
10. CoppeliaSim (formerly known as V-REP)  [[Link](https://www.coppeliarobotics.com/)]



## Foundation model for reasoning

1. Qwen2.5-VL  [[Code](https://github.com/QwenLM/Qwen2.5-VL)]
2. Bagel  [[Code](https://github.com/bytedance-seed/BAGEL)]
3. BLIP3-o  [[Code](https://github.com/JiuhaiChen/BLIP3o)]
4. Janus-Series: Unified Multimodal Understanding and Generation Models [[Code](https://github.com/deepseek-ai/Janus)]
5. Kimi-VL  [[Code](https://github.com/MoonshotAI/Kimi-VL)]
6. InternVL2-MPO [[Code](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo)]
7. Bytedance ReFT
8. Kimi-K1.5
9. DeepSeek-R1

   

## Code will be released soon

1. [arxiv 2505] WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning [[Code will be released soon](https://github.com/weizhepei/WebAgent-R1)]
2. [arxiv 2505] VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank [[Code will be released soon](https://github.com/TianheWu/VisualQuality-R1)]
3. [arxiv 2505] Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner [[Code will be released soon](https://github.com/Wenchuan-Zhang/Patho-R1)]
4. [arxiv 2505] STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs   [[Code will be released soon](https://github.com/zongzhao23/star-r1)]
5. [arxiv 2505] Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning   [[Code will be released soon](https://github.com/maifoundations/Visionary-R1)]
6. [arxiv 2505] R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO [[Code will be released soon](https://github.com/HJYao00/R1-ShareVL)]
7. [arxiv 2505] Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL [[Code will be released soon](https://cof-reasoning.github.io/)]
8. [arxiv 2505] Visual Planning: Let’s Think Only with Images [[Code will be released soon](https://github.com/yix8/VisualPlanning)]
9. [arxiv 2505] X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains [[Code will be released soon](https://github.com/microsoft/x-reasoner)]
10. [arxiv 2504] InteractVLM: 3D Interaction Reasoning from 2D Foundational Models [[Code will be released soon](https://github.com/saidwivedi/InteractVLM)]
11. [arxiv 2504] Improved Visual-Spatial Reasoning via R1-Zero-Like Training [[Code will be released soon](https://github.com/zhijie-group/R1-Zero-VSI)]
12. [arxiv 2504] Fast-Slow Thinking for Large Vision-Language Model Reasoning [[Code will be released soon](https://github.com/Mr-Loevan/FAST)]
13. [arxiv 2504] Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension [[Code will be released soon](https://github.com/HKUST-LongGroup/Relation-R1)]
14. [arxiv 2503] Q-Insight: Understanding Image Quality via Visual Reinforcement Learning [[Code will be released soon](https://github.com/lwq20020127/Q-Insight)]
15. [arxiv 2503] VisualThinker-R1-Zero: R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model [[Code will be released soon](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)]
16. [arxiv 2402] PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs [[Code will be released soon](https://pivot-prompt.github.io/)]
17. [arxiv 2312] Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models [[Code will be released soon](https://visual-program-distillation.github.io/)]



## Open-Source Projects (Repos without Paper)

### Training Framework
* [EasyR1 💻](https://github.com/hiyouga/EasyR1)  ![EasyR1](https://img.shields.io/github/stars/hiyouga/EasyR1) 

### Embodied

* [OpenVLA 💻](https://github.com/openvla/openvla) ![EasyR1](https://img.shields.io/github/stars/openvla/openvla) 
* [PI Series 💻](https://github.com/Physical-Intelligence/openpi) ![PI Series](https://img.shields.io/github/stars/Physical-Intelligence/openpi)
* [MiniVLA 💻](https://github.com/Stanford-ILIAD/openvla-mini) ![MiniVLA](https://img.shields.io/github/stars/Stanford-ILIAD/openvla-mini)

### Image

* [R1-V 💻](https://github.com/Deep-Agent/R1-V)  ![R1-V](https://img.shields.io/github/stars/Deep-Agent/R1-V) 
* [Multimodal Open R1 💻](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)  ![Multimodal Open R1](https://img.shields.io/github/stars/EvolvingLMMs-Lab/open-r1-multimodal)
* [MMR1 💻](https://github.com/LengSicong/MMR1) ![LengSicong/MMR1](https://img.shields.io/github/stars/LengSicong/MMR1) 
* [R1-Multimodal-Journey 💻](https://github.com/FanqingM/R1-Multimodal-Journey) ![R1-Multimodal-Journey](https://img.shields.io/github/stars/FanqingM/R1-Multimodal-Journey) 
* [R1-Vision 💻](https://github.com/yuyq96/R1-Vision) ![R1-Vision](https://img.shields.io/github/stars/yuyq96/R1-Vision) 
* [Ocean-R1 💻](https://github.com/VLM-RL/Ocean-R1)  ![Ocean-R1](https://img.shields.io/github/stars/VLM-RL/Ocean-R1) 
* [R1V-Free 💻](https://github.com/Exgc/R1V-Free)  ![Exgc/R1V-Free](https://img.shields.io/github/stars/Exgc/R1V-Free)
* [SeekWorld 💻](https://github.com/TheEighthDay/SeekWorld)  ![TheEighthDay/SeekWorld](https://img.shields.io/github/stars/TheEighthDay/SeekWorld) 
* [R1-Track 💻](https://github.com/Wangbiao2/R1-Track)  ![Wangbiao2/R1-Track](https://img.shields.io/github/stars/Wangbiao2/R1-Track)

### Video

* [Open R1 Video 💻](https://github.com/Wang-Xiaodong1899/Open-R1-Video) ![Open R1 Video](https://img.shields.io/github/stars/Wang-Xiaodong1899/Open-R1-Video) 
* [Temporal-R1 💻](https://github.com/appletea233/Temporal-R1)  ![Temporal-R1](https://img.shields.io/github/stars/appletea233/Temporal-R1) 
* [Open-LLaVA-Video-R1 💻](https://github.com/Hui-design/Open-LLaVA-Video-R1) ![Open-LLaVA-Video-R1](https://img.shields.io/github/stars/Hui-design/Open-LLaVA-Video-R1) 

### Agent 

* [VAGEN 💻](https://github.com/RAGEN-AI/VAGEN) ![VAGEN](https://img.shields.io/github/stars/RAGEN-AI/VAGEN)



## Acknowledgement

1. https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs
2. https://github.com/yaotingwangofficial/Awesome-MCoT
3. https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln
4. https://modelscope.cn/studios/AI-ModelScope/awesome-reasoning
5. https://github.com/Songwxuan/Embodied-AI-Paper-TopConf
6. https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List



## Citation

If you find this repository useful for your research and applications, please star us and consider citing:

```
@misc{Tang2025Awesome-spatial-visual-reasoning-MLLMs,
  title={Awesome-spatial-visual-reasoning-MLLMs},
  author={Jing Tang},
  year={2025},
  howpublished={\url{https://github.com/vaew/Awesome-spatial-visual-reasoning-MLLMs}},
  note={Github Repository},
}
```
